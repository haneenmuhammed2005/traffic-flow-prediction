# Traffic Flow Prediction for Urban Planning
## Complete Machine Learning Project

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![ML](https://img.shields.io/badge/ML-Scikit--Learn%20%7C%20XGBoost%20%7C%20LightGBM-orange)
![Status](https://img.shields.io/badge/Status-Production%20Ready-green)

A comprehensive machine learning solution for predicting traffic flow patterns to assist urban planning decisions. This project implements multiple ML algorithms, feature engineering techniques, and provides production-ready code for deployment.

---

## ğŸ“‹ Table of Contents
- [Project Overview](#project-overview)
- [Features](#features)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [Detailed Workflow](#detailed-workflow)
- [Model Performance](#model-performance)
- [Usage Examples](#usage-examples)
- [Customization](#customization)
- [Contributing](#contributing)

---

## ğŸ¯ Project Overview

This project predicts traffic volume at specific times and locations using historical traffic data combined with temporal and weather features. It's designed to help:
- **Urban Planners**: Optimize traffic light timings and road capacity
- **Transportation Authorities**: Plan maintenance during low-traffic periods
- **Commuters**: Receive traffic forecasts for route planning
- **Businesses**: Optimize delivery schedules

### Key Highlights
- **Multiple ML Models**: Random Forest, XGBoost, LightGBM, and Ensemble
- **Comprehensive Feature Engineering**: 50+ features from temporal patterns
- **Production Ready**: Clean, modular code with proper separation of concerns
- **Extensive Visualization**: 10+ plots for analysis and presentation
- **Real-time Predictions**: API-ready prediction module

---

## âœ¨ Features

### Data Processing
- Automatic missing value handling (interpolation, forward/backward fill)
- Outlier detection and treatment (IQR, Z-score methods)
- Time-based data splitting (no data leakage)
- Synthetic data generation for testing

### Feature Engineering
- **Temporal Features**: Hour, day, month, season, day of week
- **Cyclical Encoding**: Sin/cos transformations for circular time
- **Lag Features**: Previous 1h, 2h, 3h, 24h, 48h, 168h values
- **Rolling Statistics**: Mean, std, min, max over multiple windows
- **Difference Features**: First-order differences
- **Binary Indicators**: Weekend, rush hours, holidays, business hours
- **Weather Features**: Temperature bins, weather conditions
- **Interaction Features**: Complex relationships between variables

### Machine Learning Models
1. **Random Forest Regressor**
   - Robust to outliers
   - Feature importance analysis
   - Minimal hyperparameter tuning needed

2. **XGBoost**
   - Gradient boosting framework
   - Excellent performance
   - Built-in regularization

3. **LightGBM**
   - Fast training speed
   - Memory efficient
   - High accuracy

4. **Ensemble Model**
   - Weighted average of top models
   - Reduced overfitting
   - Improved generalization

### Evaluation Metrics
- MAE (Mean Absolute Error)
- RMSE (Root Mean Squared Error)
- MAPE (Mean Absolute Percentage Error)
- RÂ² Score (Coefficient of Determination)

### Visualizations
- Time series plots
- Hourly/weekly traffic patterns
- Traffic heatmaps (day Ã— hour)
- Actual vs predicted comparisons
- Residual analysis
- Feature importance charts
- Model comparison plots

---

## ğŸš€ Installation

### Prerequisites
- Python 3.8 or higher
- pip package manager
- 4GB RAM minimum (8GB recommended)

### Step 1: Clone Repository
```bash
git clone https://github.com/yourusername/traffic-flow-prediction.git
cd traffic-flow-prediction
```

### Step 2: Create Virtual Environment
```bash
# Using venv
python -m venv venv

# Activate on Linux/Mac
source venv/bin/activate

# Activate on Windows
venv\Scripts\activate
```

### Step 3: Install Dependencies
```bash
pip install -r requirements.txt
```

### Step 4: Verify Installation
```bash
python -c "import pandas, numpy, sklearn, xgboost, lightgbm; print('All dependencies installed successfully!')"
```

---

## ğŸƒ Quick Start

### Option 1: Run Complete Pipeline (Recommended for First Time)
```bash
python main_train.py
```

This will:
1. Generate synthetic traffic data
2. Perform data preprocessing
3. Create visualizations
4. Engineer features
5. Train multiple models
6. Evaluate and compare models
7. Save results and trained models

**Expected Time**: 5-10 minutes

### Option 2: Step-by-Step Execution
```bash
# 1. Generate data
python data_preprocessing.py

# 2. Create features
python feature_engineering.py

# 3. Train models
python models.py

# 4. Create visualizations
python visualization.py

# 5. Make predictions
python predict.py
```

---

## ğŸ“ Project Structure

```
traffic-flow-prediction/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                      # Raw data files
â”‚   â”‚   â””â”€â”€ traffic_data_raw.csv
â”‚   â””â”€â”€ processed/                # Processed data with features
â”‚       â””â”€â”€ traffic_data_featured.csv
â”‚
â”œâ”€â”€ models/                       # Saved trained models
â”‚   â”œâ”€â”€ traffic_model_random_forest.pkl
â”‚   â”œâ”€â”€ traffic_model_xgboost.pkl
â”‚   â””â”€â”€ traffic_model_lightgbm.pkl
â”‚
â”œâ”€â”€ plots/                        # All visualizations
â”‚   â”œâ”€â”€ 01_time_series.png
â”‚   â”œâ”€â”€ 02_hourly_pattern.png
â”‚   â”œâ”€â”€ 03_weekly_pattern.png
â”‚   â”œâ”€â”€ 04_traffic_heatmap.png
â”‚   â”œâ”€â”€ 05_predictions_*.png
â”‚   â”œâ”€â”€ 06_residuals_*.png
â”‚   â””â”€â”€ 07_feature_importance_*.png
â”‚
â”œâ”€â”€ results/                      # Model evaluation results
â”‚   â”œâ”€â”€ model_comparison.csv
â”‚   â”œâ”€â”€ predictions_next_24h.csv
â”‚   â””â”€â”€ predictions_weekly.csv
â”‚
â”œâ”€â”€ data_preprocessing.py         # Data cleaning and preprocessing
â”œâ”€â”€ feature_engineering.py        # Feature creation module
â”œâ”€â”€ models.py                     # ML models implementation
â”œâ”€â”€ visualization.py              # Plotting and visualization
â”œâ”€â”€ main_train.py                # Main training pipeline
â”œâ”€â”€ predict.py                   # Prediction module
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # This file
```

---

## ğŸ”„ Detailed Workflow

### Phase 1: Data Acquisition (Day 1 Morning)
```python
from data_preprocessing import TrafficDataPreprocessor

preprocessor = TrafficDataPreprocessor()

# Option A: Load your own data
df = preprocessor.load_data('your_traffic_data.csv')

# Option B: Generate synthetic data for testing
df = preprocessor.generate_synthetic_data(periods=8760)
```

### Phase 2: Data Preprocessing (Day 1 Afternoon)
```python
# Handle missing values
df = preprocessor.handle_missing_values(df, method='interpolate')

# Detect and treat outliers
outliers = preprocessor.detect_outliers(df, 'traffic_volume')
df = preprocessor.treat_outliers(df, 'traffic_volume', method='cap')

# Create train/val/test split
train_df, val_df, test_df = preprocessor.create_time_based_split(df)
```

### Phase 3: Feature Engineering (Day 1 Evening)
```python
from feature_engineering import TrafficFeatureEngineer

feature_engineer = TrafficFeatureEngineer()

# Create all features automatically
df_featured = feature_engineer.create_all_features(df)

# Or create specific feature groups
df = feature_engineer.extract_temporal_features(df)
df = feature_engineer.create_cyclical_features(df)
df = feature_engineer.create_lag_features(df, lags=[1, 24, 168])
df = feature_engineer.create_rolling_features(df, windows=[3, 24])
```

### Phase 4: Model Training (Day 2 Morning)
```python
from models import TrafficFlowModels, EnsembleModel

# Train single model
model = TrafficFlowModels('xgboost')
model.create_model(n_estimators=200, max_depth=6)
model.train(X_train, y_train, X_val, y_val)

# Train ensemble
ensemble = EnsembleModel(['xgboost', 'lightgbm', 'random_forest'])
ensemble.train_all(X_train, y_train, X_val, y_val)
```

### Phase 5: Evaluation (Day 2 Afternoon)
```python
from models import evaluate_model

y_pred = model.predict(X_test)
metrics = evaluate_model(y_test, y_pred)

# Metrics: MAE, RMSE, MAPE, RÂ²
```

### Phase 6: Visualization (Day 2 Evening)
```python
from visualization import TrafficVisualizer

viz = TrafficVisualizer()

viz.plot_time_series(df)
viz.plot_hourly_pattern(df)
viz.plot_predictions(y_test, y_pred, dates)
viz.plot_feature_importance(model.feature_importance)
```

### Phase 7: Predictions (Day 3)
```python
from predict import TrafficPredictor

predictor = TrafficPredictor('models/traffic_model_xgboost.pkl')

# Single prediction
traffic = predictor.predict_single_timestamp('2024-03-15 08:00')

# Next 24 hours
predictions = predictor.predict_next_n_hours(datetime.now(), 24)

# Rush hours
rush_hour_traffic = predictor.predict_rush_hours('2024-03-15')
```

---

## ğŸ“Š Model Performance

### Expected Performance (Synthetic Data)

| Model | MAE | RMSE | MAPE | RÂ² |
|-------|-----|------|------|-----|
| Random Forest | 85-100 | 110-130 | 12-15% | 0.92-0.95 |
| XGBoost | 80-95 | 105-125 | 11-14% | 0.93-0.96 |
| LightGBM | 78-92 | 102-122 | 10-13% | 0.94-0.96 |
| **Ensemble** | **75-88** | **98-118** | **9-12%** | **0.95-0.97** |

*Note: Actual performance depends on data quality and quantity*

### Performance on Real-World Data
When using real traffic data, expect:
- RÂ² scores: 0.85-0.92
- MAPE: 15-25%
- Better performance with more historical data (2+ years recommended)

---

## ğŸ’¡ Usage Examples

### Example 1: Predict Traffic for Tomorrow Morning
```python
from predict import TrafficPredictor
from datetime import datetime, timedelta

predictor = TrafficPredictor()

tomorrow_morning = datetime.now() + timedelta(days=1)
tomorrow_morning = tomorrow_morning.replace(hour=8, minute=0)

traffic = predictor.predict_single_timestamp(tomorrow_morning)
print(f"Expected traffic at 8 AM tomorrow: {traffic:.0f} vehicles")
```

### Example 2: Find Best Time for Road Maintenance
```python
# Predict traffic for entire week
start_date = datetime(2024, 3, 18)
end_date = start_date + timedelta(days=7)

weekly_pred = predictor.predict_date_range(start_date, end_date, freq='H')

# Find lowest traffic period
best_time = weekly_pred.loc[weekly_pred['predicted_traffic'].idxmin()]
print(f"Best maintenance time: {best_time['date_time']}")
print(f"Expected traffic: {best_time['predicted_traffic']:.0f}")
```

### Example 3: Compare Weekday vs Weekend Traffic
```python
predictions = predictor.predict_date_range('2024-03-18', '2024-03-24', freq='H')

weekday_avg = predictions[predictions['is_weekend'] == 0]['predicted_traffic'].mean()
weekend_avg = predictions[predictions['is_weekend'] == 1]['predicted_traffic'].mean()

print(f"Weekday average: {weekday_avg:.0f}")
print(f"Weekend average: {weekend_avg:.0f}")
print(f"Difference: {weekday_avg - weekend_avg:.0f} ({(weekday_avg/weekend_avg - 1)*100:.1f}%)")
```

---

## ğŸ”§ Customization

### Using Your Own Data

Replace the synthetic data generation with your actual data:

```python
# In main_train.py, replace this:
df = preprocessor.generate_synthetic_data(periods=8760)

# With this:
df = pd.read_csv('your_traffic_data.csv')
df['date_time'] = pd.to_datetime(df['date_time'])
```

**Required Columns**:
- `date_time`: Timestamp (datetime format)
- `traffic_volume`: Number of vehicles (numeric)
- Optional: `temperature`, `humidity`, `weather_condition`

### Adding Custom Features

```python
# In feature_engineering.py, add to TrafficFeatureEngineer class:

def create_custom_features(self, df):
    """Add your custom features"""
    df['is_school_day'] = ...
    df['special_event'] = ...
    df['road_construction'] = ...
    return df
```

### Tuning Hyperparameters

```python
from sklearn.model_selection import RandomizedSearchCV

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [5, 6, 7, 8],
    'learning_rate': [0.01, 0.05, 0.1]
}

model = xgb.XGBRegressor()
random_search = RandomizedSearchCV(model, param_grid, n_iter=20, cv=3)
random_search.fit(X_train, y_train)

print("Best parameters:", random_search.best_params_)
```

### Adding New Models

```python
# In models.py, add new model type:

elif self.model_type == 'neural_network':
    from tensorflow import keras
    self.model = keras.Sequential([
        keras.layers.Dense(128, activation='relu'),
        keras.layers.Dropout(0.2),
        keras.layers.Dense(64, activation='relu'),
        keras.layers.Dense(1)
    ])
```

---

## ğŸ“š Study Materials & Learning Path

### Day 1: Foundations
- **Python Basics** (2 hours)
  - Lists, dictionaries, functions
  - Pandas DataFrame operations
  - NumPy arrays

- **Statistics** (2 hours)
  - Mean, median, standard deviation
  - Correlation and causation
  - Outlier detection methods

- **Machine Learning Basics** (3 hours)
  - Supervised vs unsupervised learning
  - Regression vs classification
  - Train/test split concept
  - Overfitting and underfitting

**Resources**:
- Python for Data Analysis by Wes McKinney
- StatQuest YouTube channel
- Scikit-learn documentation

### Day 2: Time Series & Models
- **Time Series Analysis** (3 hours)
  - Temporal patterns
  - Seasonality and trends
  - Lag features
  - Rolling statistics

- **Tree-Based Models** (3 hours)
  - Decision trees
  - Random forests
  - Gradient boosting (XGBoost, LightGBM)

- **Model Evaluation** (2 hours)
  - Regression metrics (MAE, RMSE, RÂ²)
  - Cross-validation
  - Hyperparameter tuning

**Resources**:
- XGBoost documentation
- LightGBM tutorials
- Kaggle competitions

### Day 3: Production & Deployment
- **Code Organization** (2 hours)
  - Modular programming
  - Class design
  - Documentation

- **Model Deployment** (3 hours)
  - Model serialization (pickle, joblib)
  - API creation (Flask/FastAPI)
  - Production best practices

**Resources**:
- Clean Code by Robert Martin
- Flask/FastAPI documentation
- MLOps principles

---

## ğŸ“ Key Concepts Explained

### Why Time-Based Split?
Unlike random split, time-based split ensures:
- No data leakage (future â†’ past)
- Realistic performance evaluation
- Proper temporal validation

### Why Lag Features?
Traffic at time `t` is highly correlated with:
- Traffic at `t-1` (1 hour ago)
- Traffic at `t-24` (same time yesterday)
- Traffic at `t-168` (same time last week)

### Why Cyclical Encoding?
Hour 23 and hour 0 are actually close in time, but their numeric difference is 23. Sin/cos encoding captures this circular relationship.

### Why Ensemble?
Combining multiple models:
- Reduces individual model bias
- Improves generalization
- Provides more robust predictions

---

## ğŸ› Troubleshooting

### Issue: "Memory Error" during training
**Solution**: Reduce data size or use LightGBM
```python
# Sample data
df = df.sample(frac=0.5)

# Or use LightGBM (more memory efficient)
model = TrafficFlowModels('lightgbm')
```

### Issue: "Poor model performance"
**Solutions**:
1. Check data quality (missing values, outliers)
2. Increase training data size
3. Add more relevant features
4. Tune hyperparameters
5. Try ensemble model

### Issue: "Predictions are negative"
**Solution**: Add non-negativity constraint
```python
predictions = np.maximum(predictions, 0)
```

### Issue: "Training takes too long"
**Solutions**:
```python
# Reduce n_estimators
model.create_model(n_estimators=50)  # Instead of 200

# Use fewer features
top_features = feature_importance.head(30)['feature'].tolist()
X_train_subset = X_train[top_features]
```

---

## ğŸ“ˆ Next Steps & Improvements

### Short-term (Week 1-2)
- [ ] Collect real traffic data
- [ ] Add weather API integration
- [ ] Create simple web interface
- [ ] Deploy model to cloud

### Medium-term (Month 1-2)
- [ ] Implement LSTM/GRU for deep learning
- [ ] Add real-time prediction API
- [ ] Create interactive dashboard
- [ ] A/B test different models

### Long-term (Month 3+)
- [ ] Multi-location prediction
- [ ] Traffic anomaly detection
- [ ] Route optimization algorithm
- [ ] Mobile app integration

---

## ğŸ¤ Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests if applicable
5. Submit a pull request

---

## ğŸ“ License

This project is licensed under the MIT License - see LICENSE file for details.

---

## ğŸ“§ Contact

For questions or collaboration:
- Email: your.email@example.com
- LinkedIn: [Your Profile]
- GitHub: [@yourusername]

---

## ğŸ™ Acknowledgments

- Scikit-learn team for excellent ML library
- XGBoost and LightGBM developers
- Open-source community
- Urban planning research papers

---

## ğŸ“š References

1. Scikit-learn Documentation: https://scikit-learn.org/
2. XGBoost Documentation: https://xgboost.readthedocs.io/
3. LightGBM Documentation: https://lightgbm.readthedocs.io/
4. Time Series Analysis: https://otexts.com/fpp3/
5. Feature Engineering for ML: http://www.feat.engineering/

---

**Built with â¤ï¸ for Urban Planning and Smart Cities**

Last Updated: 2024
Version: 1.0.0
